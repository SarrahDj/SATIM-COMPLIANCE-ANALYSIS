{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational Agent for Legal and Regulatory Information\n",
        "\n",
        "This Notebook sets up a Retrieval Augmented Generation (RAG) chatbot using **LangChain**, **Hugging Face models**, and **ChromaDB**. This chatbot is designed to answer questions based on a provided knowledge base of **PDF**, **DOCX**, and **TXT** documents of internal security policies and regulations.\n",
        "\n",
        "---\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project develops a conversational agent (chatbot) designed to answer specific questions about regulations and laws. It leverages a **Retrieval Augmented Generation (RAG)** architecture to ensure accurate and up-to-date responses by referencing a dedicated knowledge base of documents.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "- Initializes LLM and QA chain\n",
        "- Includes checks to ensure everything is correctly configured\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5CMAmffxtINb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "folder_name = \"knowledge\"\n",
        "os.makedirs(folder_name, exist_ok=True)"
      ],
      "metadata": {
        "id": "6Atmjw6WtEhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community chromadb tiktoken pydantic==1.10.13 python-dotenv\n",
        "!pip install transformers accelerate bitsandbytes torch sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install python-docx\n",
        "!pip install gradio\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "id": "FLiYtCPMx5UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "0r7eR2HZx-I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "DATA_PATH = \"knowledge\"\n",
        "CHROMA_DB_PATH = \"chroma_db\"\n",
        "\n",
        "# Models used\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL_NAME = \"google/gemma-2b-it\"\n",
        "\n",
        "\n",
        "qa_chain = None\n",
        "vector_store = None\n",
        "\n",
        "# --- 1. Load Documents ---\n",
        "def load_documents(data_path: str):\n",
        "    print(f\"Loading documents from: {data_path}\")\n",
        "    documents = []\n",
        "    pdf_loader = DirectoryLoader(data_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
        "    pdf_docs = pdf_loader.load()\n",
        "    print(f\"Loaded {len(pdf_docs)} PDF documents.\")\n",
        "    documents.extend(pdf_docs)\n",
        "\n",
        "    docx_loader = DirectoryLoader(data_path, glob=\"**/*.docx\", loader_cls=Docx2txtLoader)\n",
        "    docx_docs = docx_loader.load()\n",
        "    print(f\"Loaded {len(docx_docs)} DOCX documents.\")\n",
        "    documents.extend(docx_docs)\n",
        "\n",
        "    txt_loader = DirectoryLoader(data_path, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "    txt_docs = txt_loader.load()\n",
        "    print(f\"Loaded {len(txt_docs)} TXT documents.\")\n",
        "    documents.extend(txt_docs)\n",
        "\n",
        "    print(f\"Total loaded documents: {len(documents)}\")\n",
        "    if not documents:\n",
        "        print(\"DEBUG: No documents were loaded from any supported format. Please check DATA_PATH and file types.\")\n",
        "    else:\n",
        "        print(f\"DEBUG: First 200 chars of first loaded doc: {documents[0].page_content[:200]}...\")\n",
        "    return documents\n",
        "\n",
        "# --- 2. Chunk Documents ---\n",
        "def chunk_documents(documents):\n",
        "    print(\"Splitting documents into chunks...\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Created {len(chunks)} chunks.\")\n",
        "    if not chunks:\n",
        "        print(\"DEBUG: No chunks were created. This means either no documents loaded, or documents were empty/too small.\")\n",
        "    return chunks\n",
        "\n",
        "# --- 3. Create Embeddings & Store in Vector DB ---\n",
        "def setup_vector_store(chunks, db_path: str):\n",
        "    print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'})\n",
        "\n",
        "    if os.path.exists(db_path) and os.listdir(db_path):\n",
        "        print(\"ChromaDB already exists. Attempting to load existing collection.\")\n",
        "        try:\n",
        "            current_vector_store = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "            if current_vector_store._collection.count() > 0:\n",
        "                print(f\"Loaded {current_vector_store._collection.count()} embeddings from existing DB.\")\n",
        "                return current_vector_store\n",
        "            else:\n",
        "                print(\"Existing ChromaDB is empty or invalid. Re-creating.\")\n",
        "                current_vector_store = Chroma.from_documents(\n",
        "                    documents=chunks,\n",
        "                    embedding=embeddings,\n",
        "                    persist_directory=db_path\n",
        "                )\n",
        "                print(\"New ChromaDB created and populated.\")\n",
        "                return current_vector_store\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing ChromaDB: {e}. Re-creating.\")\n",
        "            current_vector_store = Chroma.from_documents(\n",
        "                documents=chunks,\n",
        "                embedding=embeddings,\n",
        "                persist_directory=db_path\n",
        "            )\n",
        "            print(\"New ChromaDB created and populated.\")\n",
        "            return current_vector_store\n",
        "    else:\n",
        "        print(\"ChromaDB not found or empty. Creating new database.\")\n",
        "        current_vector_store = Chroma.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings,\n",
        "            persist_directory=db_path\n",
        "        )\n",
        "        print(\"New ChromaDB created and populated.\")\n",
        "        return current_vector_store\n",
        "\n",
        "# --- 4. Setup LLM and RetrievalQA Chain ---\n",
        "def setup_qa_chain(current_vector_store):\n",
        "    print(f\"Loading LLM: {LLM_MODEL_NAME}. This may take a few minutes...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True,\n",
        "\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=500,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    print(\"Setting up RetrievalQA chain...\")\n",
        "    retriever = current_vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "    qa_chain_instance = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        return_source_documents=True\n",
        "    )\n",
        "    print(\"RetrievalQA chain ready.\")\n",
        "    return qa_chain_instance\n",
        "\n",
        "# --- Initialize Chatbot Components ---\n",
        "def initialize_chatbot():\n",
        "    global qa_chain, vector_store\n",
        "    if qa_chain is None:\n",
        "        print(\"\\n--- Initializing Chatbot Components ---\")\n",
        "        documents = load_documents(DATA_PATH)\n",
        "        if not documents:\n",
        "            print(\"Initialization failed: No documents loaded. Please check your knowledge_base folder and files.\")\n",
        "            return None\n",
        "        chunks = chunk_documents(documents)\n",
        "        if not chunks:\n",
        "            print(\"Initialization failed: No chunks created. Ensure documents are not empty.\")\n",
        "            return None\n",
        "        vector_store = setup_vector_store(chunks, CHROMA_DB_PATH)\n",
        "        qa_chain = setup_qa_chain(vector_store)\n",
        "        print(\"--- Chatbot Initialization Complete ---\")\n",
        "    else:\n",
        "        print(\"Chatbot already initialized.\")\n",
        "    return qa_chain\n",
        "\n",
        "# --- Gradio Interface Function---\n",
        "def chat_interface(user_query):\n",
        "    global qa_chain\n",
        "    if qa_chain is None:\n",
        "        return \"Chatbot is still initializing. Please wait a moment and try again.\"\n",
        "\n",
        "    print(f\"\\nUser Query: {user_query}\")\n",
        "    try:\n",
        "        result = qa_chain({\"query\": user_query})\n",
        "        full_llm_response = result[\"result\"]\n",
        "\n",
        "        helpful_answer_marker = \"Helpful Answer:\"\n",
        "        marker_index = full_llm_response.find(helpful_answer_marker)\n",
        "\n",
        "        answer = \"\"\n",
        "        if marker_index != -1:\n",
        "            answer = full_llm_response[marker_index + len(helpful_answer_marker):].strip()\n",
        "            answer_start_of_next_section = answer.find(\"**Answer:\")\n",
        "            if answer_start_of_next_section != -1:\n",
        "                answer = answer[:answer_start_of_next_section].strip()\n",
        "        else:\n",
        "            answer = full_llm_response.replace(f\"<bos>{LLM_MODEL_NAME.split('/')[-1]}\\n\", \"\").strip()\n",
        "\n",
        "\n",
        "        lower_answer = answer.lower()\n",
        "        unhelpful_phrases = [\n",
        "            \"i don't know\",\n",
        "            \"i cannot answer\",\n",
        "            \"no information found\",\n",
        "            \"based on the provided information, i cannot answer\",\n",
        "            \"the provided context does not contain information\",\n",
        "            \"i am unable to answer\",\n",
        "            \"i can't find the answer\",\n",
        "            \"not mentioned in the provided text\",\n",
        "            \"i am just a language model\"\n",
        "        ]\n",
        "\n",
        "        for phrase in unhelpful_phrases:\n",
        "            if phrase in lower_answer:\n",
        "                return \"I apologize, but I couldn't find a helpful answer to your question in the documents I have access to.\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An internal error occurred during response generation: {e}\")\n",
        "        return \"I'm sorry, an internal issue prevented me from answering your question. Please try again or ask a different question.\"\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    initialized_qa_chain = initialize_chatbot()\n",
        "\n",
        "    if initialized_qa_chain:\n",
        "        print(\"\\n--- Launching Gradio Interface ---\")\n",
        "        iface = gr.Interface(\n",
        "            fn=chat_interface,\n",
        "            inputs=gr.Textbox(lines=2, placeholder=\"Ask me a question about your documents...\"),\n",
        "            outputs=\"textbox\",\n",
        "            title=\"RAG Chatbot Demo (powered by Hugging Face & LangChain)\",\n",
        "            description=\"Ask questions based on the documents you provided. This demo uses a local Gemma-2b-it LLM.\",\n",
        "            live=False\n",
        "        )\n",
        "        iface.launch(share=True, debug=True)\n",
        "    else:\n",
        "        print(\"\\nGradio interface not launched due to chatbot initialization failure.\")"
      ],
      "metadata": {
        "id": "meFFqpo0yUq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Summary\n",
        "\n",
        "This notebook provides a full pipeline for building a **RAG-based chatbot** that:\n",
        "- Loads and processes legal documents\n",
        "- Converts them into embeddings\n",
        "- Answers user queries through a web interface\n",
        "- Provides helpful and context-aware legal/regulatory answers\n"
      ],
      "metadata": {
        "id": "Y6uFKt2z2E44"
      }
    }
  ]
}